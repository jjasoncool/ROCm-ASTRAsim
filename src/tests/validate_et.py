#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ET æª”æ¡ˆé©—è­‰å·¥å…· - å¢å¼·ç‰ˆ

ç”¨é€”ï¼š
  é€æª”é©—è­‰ Chakra protobuf .etï¼ˆExecution Traceï¼‰æª”æ¡ˆçš„æ­£ç¢ºæ€§ï¼Œä¸¦æä¾›è©³ç´°åˆ†æã€‚

ä¸»è¦åŠŸèƒ½ï¼š
  - åŸºæœ¬é©—è­‰ï¼šæª”æ¡ˆæ ¼å¼ã€protobuf è§£ç¢¼ã€metadata ç‰ˆæœ¬
  - è©³ç´°åˆ†æï¼šç¯€é»é¡å‹çµ±è¨ˆã€é€šè¨Šå¤§å°ã€åŸ·è¡Œæ™‚é–“åˆ†æ
  - DAG ä¾è³´é©—è­‰ï¼šæª¢æŸ¥ç¯€é»ä¾è³´é—œä¿‚çš„å®Œæ•´æ€§å’Œåˆç†æ€§
  - æ·±åº¦é©—è­‰ï¼šè·¨ rank ä¸€è‡´æ€§æª¢æŸ¥ã€ç¯€é»å°æ‡‰é—œä¿‚é©—è­‰
  - éŒ¯èª¤æª¢æ¸¬ï¼šæª”æ¡ˆæˆªæ–·ã€æ ¼å¼éŒ¯èª¤ã€æ•¸æ“šä¸ä¸€è‡´
  - è‡ªå‹•ä¿®å¾©ï¼šç”Ÿæˆä¹¾æ·¨çš„ ET æª”æ¡ˆ

é—œéµç‰¹æ€§ï¼š
  1) å®‰å…¨çš„ protobuf è§£ç¢¼ï¼šé¿å…åœ¨æˆªæ–·æª”æ¡ˆè™•ç„¡é™ç­‰å¾…
  2) å¤šå±¤æ¬¡é©—è­‰ï¼šå¾åŸºæœ¬æ ¼å¼åˆ°æ·±åº¦èªç¾©æª¢æŸ¥
  3) DAG å®Œæ•´æ€§æª¢æŸ¥ï¼šè‡ªæˆ‘ä¾è³´ã€ç¼ºå¤±ç¯€é»ã€å¾ªç’°ä¾è³´æª¢æ¸¬
  4) è©³ç´°çµ±è¨ˆï¼šé€šè¨Šæ¨¡å¼ã€æ™‚é–“åˆ†å¸ƒã€ç¯€é»é¡å‹åˆ†æ
  5) è·¨ rank é©—è­‰ï¼šç¢ºä¿åˆ†æ•£å¼è¨“ç·´çš„ä¸€è‡´æ€§
  6) ä¿®å¾©åŠŸèƒ½ï¼šè‡ªå‹•ç”Ÿæˆä¿®å¾©å¾Œçš„å·¥ä½œè² è¼‰
  7) é€šç”¨æª”åæ”¯æ´ï¼šæ”¯æ´ä»»ä½• prefix.{num}.et æ ¼å¼çš„æª”æ¡ˆ

ä½¿ç”¨æ–¹å¼ï¼š
  åŸºæœ¬é©—è­‰ï¼ˆå«è©³ç´°åˆ†æï¼‰ï¼š
    python src/tests/validate_et.py

  æ·±åº¦é©—è­‰ï¼š
    python src/tests/validate_et.py --validate

  DAG ä¾è³´æª¢æŸ¥ï¼š
    python src/tests/validate_et.py --check-dag

  è‡ªå‹•ä¿®å¾©ï¼š
    python src/tests/validate_et.py --fix --output data/chakra/fixed_workload

  é™åˆ¶ç¯€é»æ•¸ï¼ˆå¿«é€Ÿæª¢æŸ¥ï¼‰ï¼š
    python src/tests/validate_et.py --max-nodes 20

  å®Œæ•´åˆ†æï¼š
    python src/tests/validate_et.py --validate --check-dag

æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š
  - allreduce.0.et, allreduce.1.et, ...
  - et.0.et, et.1.et, ...
  - workload.0.et, workload.1.et, ...
  - ä»»ä½• prefix.{rank}.et æ ¼å¼

è¼¸å‡ºèªªæ˜ï¼š
  ğŸ“ æª”æ¡ˆåŸºæœ¬è³‡è¨Š
  ğŸ” è©³ç´°ç¯€é»åˆ†æ
  ğŸ”— DAG ä¾è³´é—œä¿‚æª¢æŸ¥
  ğŸ› ï¸  è‡ªå‹•ä¿®å¾©åŠŸèƒ½
  âœ… é©—è­‰é€šéé …ç›®
  âš ï¸  æ½›åœ¨å•é¡Œè­¦å‘Š
  âŒ åš´é‡éŒ¯èª¤é …ç›®
"""

import sys
import re
import argparse
import signal
import time
import json
import subprocess
from pathlib import Path
from collections import defaultdict
from typing import Dict, Any, List, Tuple, Optional, Set

# Chakra çš„ protolibï¼šæä¾› encode/decodeMessageï¼ˆé•·åº¦å‰ç¶´çš„ protobuf ä¸²æµï¼‰
from chakra.src.third_party.utils.protolib import decodeMessage as decode_message
# et_def_pb2ï¼šChakra çš„ protobuf schemaï¼ˆNode / GlobalMetadata / å¸¸æ•¸ï¼‰
from chakra.schema.protobuf.et_def_pb2 import (
    GlobalMetadata, Node, ALL_REDUCE, ALL_GATHER, REDUCE_SCATTER, ALL_TO_ALL
)

COMM_NAMES = {
    ALL_REDUCE: "ALL_REDUCE",
    ALL_GATHER: "ALL_GATHER",
    REDUCE_SCATTER: "REDUCE_SCATTER",
    ALL_TO_ALL: "ALL_TO_ALL",
}

# åŒ¹é… prefix.{num}.et æª”åï¼ˆç¢ºä¿æ’åºèˆ‡ rank ä¸€è‡´ï¼‰
PATTERN = re.compile(r"^(.+)\.(\d+)\.et$")


def read_one_et(p: Path, max_nodes: int | None = None, detailed: bool = False, return_nodes: bool = False):
    """
    è§£ä¸€å€‹ .et æª”ä¸¦å›å‚³ (meta, nodes_count/nodes_list, total_bytes, kinds, detailed_info)ã€‚

    Args:
        return_nodes: å¦‚æœç‚º Trueï¼Œè¿”å›ç¯€é»åˆ—è¡¨ï¼›å¦å‰‡è¿”å›ç¯€é»æ•¸é‡ï¼ˆå‘å¾Œå…¼å®¹ï¼‰

    å®‰å…¨æ€§è¦é»ï¼š
      - å…ˆè¨˜æª”æ¡ˆç¸½é•·åº¦ sizeï¼Œåƒ…åœ¨ f.tell() < size æ™‚è§£ä¸‹ä¸€ç­†ï¼Œé¿å…åœ¨ EOF è™• decode è€Œå¡ä½ã€‚
      - æ¯ç­†è§£å®Œå¾Œæª¢æŸ¥ä½ç§»æ˜¯å¦å‰é€²ï¼ˆpos1 > pos0ï¼‰ï¼Œè‹¥æœªå‰é€²è¦–ç‚ºå£è³‡æ–™ç›´æ¥ä¸ŸéŒ¯ã€‚
    """
    size = p.stat().st_size
    detailed_info = {
        'compute_nodes': 0,
        'comm_nodes': 0,
        'alpha_nodes': 0,
        'timing_info': [],
        'node_types': {},
        'comm_sizes': []
    }

    with p.open("rb") as f:
        # å…ˆè®€ GlobalMetadataï¼ˆé–‹é ­ä¸€å®šæœ‰ï¼‰
        meta = GlobalMetadata()
        before = f.tell()
        decode_message(f, meta)         # å…§éƒ¨æœƒå…ˆè®€ varint32 é•·åº¦ï¼Œå†è®€å°æ‡‰ bytes
        after = f.tell()
        if after == before:
            raise RuntimeError(f"{p.name}: ç„¡æ³•è®€å– GlobalMetadataï¼ˆæª”æ¡ˆæ ¼å¼ä¸å°æˆ–ç‚ºç©ºï¼‰")

        nodes_count = 0
        nodes_list = []
        total_bytes = 0
        kinds: dict[int, int] = {}

        # é€ Node è®€å–ï¼šä»¥ã€Œæª”æ¡ˆå¤§å°ã€ç‚ºä¸Šé™ï¼Œé¿å…åœ¨ EOF æˆ–å£å°¾å·´è™•ç„¡é™è®€å–
        while f.tell() < size:
            if max_nodes is not None and nodes_count >= max_nodes:
                break

            pos0 = f.tell()
            node = Node()
            try:
                decode_message(f, node)  # ä¸€æ¨£ï¼šå…ˆè®€é•·åº¦å‰ç¶´ï¼Œå†è®€å…§å®¹
            except Exception:
                # è®€åˆ°å£ç¯€é» / éé æœŸ EOFï¼šç›´æ¥è·³å‡ºï¼ˆå·²è®€åˆ°çš„è³‡æ–™ä»å¯å›å ±ï¼‰
                break
            pos1 = f.tell()

            # é˜²å‘†ï¼šè§£å®Œä¸€ç­†å¾Œ offset æ‡‰è©²å‰é€²ï¼›å¦å‰‡è¡¨ç¤ºæª”æ¡ˆå£æ‰æˆ– decode å‡ºéŒ¯ï¼Œé¿å…å¡æ­»
            if pos1 <= pos0:
                raise RuntimeError(f"{p.name}: è§£ç¢¼å¡ä½ï¼ˆoffset æœªå‰é€² at {pos0}ï¼‰ï¼Œæª”æ¡ˆå¯èƒ½æ¯€æ")

            nodes_count += 1
            if return_nodes:
                nodes_list.append(node)

            # æŠ½å–å¸¸ç”¨å±¬æ€§ï¼ˆcomm_type / comm_sizeï¼‰åšçµ±è¨ˆæ‘˜è¦
            comm_type = None
            comm_size = None
            alpha = None
            duration_micros = None

            for a in node.attr:
                if a.name == "comm_type":
                    comm_type = a.int64_val
                elif a.name == "comm_size":
                    comm_size = a.int64_val
                elif a.name == "alpha":
                    alpha = a.float_val
                elif a.name == "duration_micros":
                    duration_micros = a.float_val

            # è©³ç´°åˆ†æ
            if detailed:
                # ç¯€é»é¡å‹åˆ†æ
                node_type = node.type if hasattr(node, 'type') else 'UNKNOWN'
                detailed_info['node_types'][node_type] = detailed_info['node_types'].get(node_type, 0) + 1

                # è¨ˆç®—/é€šè¨Šç¯€é»åˆ†é¡
                if comm_type is not None:
                    detailed_info['comm_nodes'] += 1
                    if comm_size is not None:
                        detailed_info['comm_sizes'].append(comm_size)
                else:
                    detailed_info['compute_nodes'] += 1

                # Alpha å€¼æª¢æŸ¥
                if alpha is not None:
                    detailed_info['alpha_nodes'] += 1

                # æ™‚é–“è³‡è¨Šï¼ˆä¿®æ­£ï¼šè¨˜éŒ„ node.idï¼‰
                if duration_micros is not None:
                    detailed_info['timing_info'].append({
                        'node_id': node.id,
                        'duration': duration_micros,
                        'type': 'COMM' if comm_type is not None else 'COMPUTE'
                    })

            if comm_type is not None:
                kinds[comm_type] = kinds.get(comm_type, 0) + 1
            if comm_size is not None:
                total_bytes += comm_size

        return meta, (nodes_list if return_nodes else nodes_count), total_bytes, kinds, detailed_info if detailed else None


def check_dag_integrity(nodes: List[Node]) -> Tuple[bool, List[str]]:
    """
    æª¢æŸ¥ DAG çš„å®Œæ•´æ€§ï¼ŒåŒ…æ‹¬ï¼š
    1. è‡ªæˆ‘ä¾è³´æª¢æŸ¥
    2. ç¼ºå¤±ç¯€é»æª¢æŸ¥
    3. ä¾è³´é—œä¿‚çš„åˆç†æ€§

    Returns:
        (is_valid, issues_list)
    """
    issues = []

    # å»ºç«‹ç¯€é» ID åˆ°ç´¢å¼•çš„æ˜ å°„
    id_to_index = {node.id: i for i, node in enumerate(nodes)}

    # 1. æª¢æŸ¥è‡ªæˆ‘ä¾è³´
    self_deps = []
    for i, node in enumerate(nodes):
        if hasattr(node, 'ctrl_deps') and node.id in node.ctrl_deps:
            self_deps.append(f"Node {node.id} (index {i}) has self-dependency")

    if self_deps:
        issues.extend(self_deps)
        issues.append(f"âŒ Found {len(self_deps)} nodes with self-dependencies")

    # 2. æª¢æŸ¥ç¼ºå¤±çš„ä¾è³´ç¯€é»
    missing_deps = []
    for i, node in enumerate(nodes):
        if hasattr(node, 'ctrl_deps'):
            for dep_id in node.ctrl_deps:
                if dep_id not in id_to_index:
                    missing_deps.append(f"Node {node.id} depends on missing node {dep_id}")

    if missing_deps:
        issues.extend(missing_deps)
        issues.append(f"âŒ Found {len(missing_deps)} missing dependency references")

    # 3. æª¢æŸ¥å¾ªç’°ä¾è³´ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    visited = set()
    rec_stack = set()

    def has_cycle(node_id: int) -> bool:
        if node_id in rec_stack:
            return True
        if node_id in visited or node_id not in id_to_index:
            return False

        visited.add(node_id)
        rec_stack.add(node_id)

        node = nodes[id_to_index[node_id]]
        if hasattr(node, 'ctrl_deps'):
            for dep_id in node.ctrl_deps:
                if dep_id in id_to_index and has_cycle(dep_id):
                    return True

        rec_stack.remove(node_id)
        return False

    cycles_found = []
    for node in nodes:
        if node.id not in visited:
            if has_cycle(node.id):
                cycles_found.append(f"Cycle detected involving node {node.id}")

    if cycles_found:
        issues.extend(cycles_found)
        issues.append(f"âŒ Found potential cycles in dependency graph")

    is_valid = len(issues) == 0
    return is_valid, issues


def fix_et_files(output_dir: str) -> bool:
    """
    è‡ªå‹•ä¿®å¾© ET æª”æ¡ˆçš„å•é¡Œï¼Œç”Ÿæˆä¹¾æ·¨çš„å·¥ä½œè² è¼‰

    Returns:
        True if successful, False otherwise
    """
    print("ğŸ› ï¸  é–‹å§‹è‡ªå‹•ä¿®å¾© ET æª”æ¡ˆ...")

    try:
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # å°‹æ‰¾ host trace æª”æ¡ˆ
        host_files = list(Path("data/chakra").glob("*host*.et"))
        if not host_files:
            print("âŒ æ‰¾ä¸åˆ° host trace æª”æ¡ˆ")
            return False

        print(f"ğŸ“ æ‰¾åˆ° {len(host_files)} å€‹ host trace æª”æ¡ˆ")

        # ä½¿ç”¨ chakra_converter ç›´æ¥è½‰æ›
        for host_file in host_files:
            rank = "0"  # é è¨­ rank
            if "rank" in host_file.name:
                import re
                match = re.search(r'rank(\d+)', host_file.name)
                if match:
                    rank = match.group(1)

            output_file = output_path / f"allreduce.{rank}.et"

            cmd = [
                "python", "-m", "chakra.et_converter.et_converter",
                "--input_type", "PyTorchProfiler",
                "--input_filename", str(host_file),
                "--output_filename", str(output_file),
                "--output_type", "Chakra"
            ]

            print(f"ğŸ”„ è½‰æ› {host_file.name} -> {output_file.name}")
            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {output_file.name}")
            else:
                print(f"âŒ è½‰æ›å¤±æ•—: {result.stderr}")
                return False

        print(f"ğŸ‰ ä¿®å¾©å®Œæˆï¼æª”æ¡ˆå„²å­˜è‡³: {output_dir}")
        return True

    except Exception as e:
        print(f"âŒ ä¿®å¾©éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--max-nodes", type=int, default=None,
        help="æ¯æª”æœ€å¤šè§£ç¢¼çš„ç¯€é»æ•¸ï¼ˆæŠ½æ¨£é©—è­‰ï¼›é è¨­å…¨é‡ï¼‰"
    )
    ap.add_argument(
        "--validate", action="store_true",
        help="åŸ·è¡Œæ·±åº¦é©—è­‰æª¢æŸ¥ï¼ˆæª¢æŸ¥ç¯€é»ä¸€è‡´æ€§ã€æ™‚åºç­‰ï¼‰"
    )
    ap.add_argument(
        "--check-dag", action="store_true",
        help="åŸ·è¡Œ DAG ä¾è³´é—œä¿‚å®Œæ•´æ€§æª¢æŸ¥"
    )
    ap.add_argument(
        "--fix", action="store_true",
        help="è‡ªå‹•ä¿®å¾© ET æª”æ¡ˆå•é¡Œ"
    )
    ap.add_argument(
        "--output", type=str, default="data/chakra/fixed_workload",
        help="ä¿®å¾©å¾Œæª”æ¡ˆçš„è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ï¼šdata/chakra/fixed_workloadï¼‰"
    )
    args = ap.parse_args()

    # å¦‚æœæ˜¯ä¿®å¾©æ¨¡å¼ï¼Œç›´æ¥åŸ·è¡Œä¿®å¾©
    if args.fix:
        success = fix_et_files(args.output)
        sys.exit(0 if success else 1)

    # è©³ç´°åˆ†æé è¨­é–‹å•Ÿ
    detailed = True

    # ä»¥ã€Œæ­¤ç¨‹å¼æ‰€åœ¨ç›®éŒ„ã€ç‚ºåŸºæº–å®šä½åˆ° ../../data/chakra/workload_et
    script_dir = Path(__file__).resolve().parent
    trace_dir = (script_dir / "../../data/chakra/workload_et").resolve()

    # æ‰¾æ‰€æœ‰ prefix.{num}.et æª”æ¡ˆï¼Œä¸¦ä¾ {num} æ•¸å€¼æ’åºï¼ˆå°æ‡‰ rankï¼‰
    all_files = list(trace_dir.glob("*.et"))
    files = [p for p in all_files if PATTERN.match(p.name)]
    files.sort(key=lambda p: int(PATTERN.match(p.name).group(2)))  # group(2) æ˜¯æ•¸å­—éƒ¨åˆ†

    if not files:
        print(f"[ERR] æ‰¾ä¸åˆ°ç¬¦åˆæ¨£å¼çš„æª”æ¡ˆï¼š{trace_dir}/*.{{num}}.et")
        sys.exit(1)

    # æª¢æŸ¥æª”æ¡ˆå‰ç¶´ä¸€è‡´æ€§
    prefixes = set(PATTERN.match(p.name).group(1) for p in files)
    if len(prefixes) > 1:
        print(f"[WARN] ç™¼ç¾å¤šç¨®æª”æ¡ˆå‰ç¶´ï¼š{sorted(prefixes)}")
    else:
        prefix = list(prefixes)[0]
        print(f"[INFO] æª¢æŸ¥ç›®éŒ„ï¼š{trace_dir}  æª”æ•¸ï¼š{len(files)}  å‰ç¶´ï¼š{prefix}")

    all_detailed_info = []

    for p in files:
        meta, nodes, total_bytes, kinds, detailed_info = read_one_et(
            p, max_nodes=args.max_nodes, detailed=detailed or args.validate
        )

        # é‡æ–°è®€å–å®Œæ•´ç¯€é»è³‡æ–™ç”¨æ–¼ DAG æª¢æŸ¥
        dag_nodes = None
        if args.check_dag:
            _, dag_nodes, _, _, _ = read_one_et(p, max_nodes=None, detailed=False, return_nodes=True)

        kind_str = ", ".join([f"{COMM_NAMES.get(k, k)}:{v}" for k, v in kinds.items()]) or "N/A"

        rank = int(PATTERN.match(p.name).group(2))  # group(2) æ˜¯æ•¸å­—éƒ¨åˆ†
        print(f"ğŸ“ {p.name:20s}  version={meta.version or 'N/A'}  nodes={nodes:4d}  bytes_sum={total_bytes:,}  kinds=({kind_str})")

        # ç‰ˆæœ¬æé†’ï¼ˆåƒ…æç¤ºï¼Œéè‡´å‘½ï¼‰
        if meta.version and meta.version not in ["0.0.4", "0.0.3", "0.0.x"]:
            print(f"  [WARN] æœªçŸ¥çš„ metadata ç‰ˆæœ¬ï¼š{meta.version}")

        # DAG ä¾è³´é—œä¿‚æª¢æŸ¥
        if args.check_dag and dag_nodes:
            print(f"   ğŸ”— DAG ä¾è³´é—œä¿‚æª¢æŸ¥ (Rank {rank}):")
            dag_valid, dag_issues = check_dag_integrity(dag_nodes)
            if dag_valid:
                print(f"      âœ… DAG çµæ§‹æ­£å¸¸")
            else:
                print(f"      âŒ ç™¼ç¾ DAG å•é¡Œ:")
                for issue in dag_issues[:5]:  # é™åˆ¶é¡¯ç¤ºå‰ 5 å€‹å•é¡Œ
                    print(f"         â€¢ {issue}")
                if len(dag_issues) > 5:
                    print(f"         ... é‚„æœ‰ {len(dag_issues)-5} å€‹å•é¡Œ")

        # è©³ç´°åˆ†æ (é è¨­é–‹å•Ÿ)
        if detailed and detailed_info:
            print(f"   ğŸ” è©³ç´°åˆ†æ (Rank {rank}):")
            print(f"      è¨ˆç®—ç¯€é»: {detailed_info['compute_nodes']}")
            print(f"      é€šè¨Šç¯€é»: {detailed_info['comm_nodes']}")
            print(f"      å«Alphaç¯€é»: {detailed_info['alpha_nodes']}")

            if detailed_info['comm_sizes']:
                avg_comm = sum(detailed_info['comm_sizes']) / len(detailed_info['comm_sizes'])
                print(f"      å¹³å‡é€šè¨Šå¤§å°: {avg_comm:,.0f} bytes ({avg_comm/1024/1024:.1f} MB)")

            if detailed_info['timing_info']:
                total_time = sum(t['duration'] for t in detailed_info['timing_info'])
                compute_time = sum(t['duration'] for t in detailed_info['timing_info'] if t['type'] == 'COMPUTE')
                comm_time = sum(t['duration'] for t in detailed_info['timing_info'] if t['type'] == 'COMM')
                print(f"      ç¸½åŸ·è¡Œæ™‚é–“: {total_time:,.0f} Î¼s")
                print(f"      è¨ˆç®—æ™‚é–“: {compute_time:,.0f} Î¼s ({compute_time/total_time*100:.1f}%)")
                print(f"      é€šè¨Šæ™‚é–“: {comm_time:,.0f} Î¼s ({comm_time/total_time*100:.1f}%)")

            # === ASTRA ç›¸å®¹æ€§å¥æª¢ï¼ˆæ–°å¢è¼¸å‡ºï¼Œä¸æ”¹å‹•åŸæœ‰è¨»è§£ï¼‰ ===
            # 1) åµæ¸¬å¯èƒ½æ··å…¥æ’ç¨‹çš„ METADATA/PG ç¯€é»
            raw_bytes = p.read_bytes()
            node_types_map = detailed_info.get('node_types', {})
            # å¸¸è¦‹æƒ…æ³ï¼šschema ä¸‹çš„ METADATA_NODE æœƒæ˜¯ type=1ï¼ˆä»¥ä½ ç’°å¢ƒç‚ºæº–ï¼‰
            suspicious_meta = (1 in node_types_map) or (b"process_group" in raw_bytes or b"## process_group" in raw_bytes)
            if suspicious_meta:
                print("      âŒ åµæ¸¬åˆ°å¯èƒ½çš„ METADATA/PG äº‹ä»¶æ··å…¥å¯ç™¼ä½ˆç¯€é»ï¼›è«‹åœ¨è½‰æª”å¾Œæ¸…ç†ï¼ˆfix_et_dag_inplace çš„ ASTRA ç›¸å®¹æµç¨‹ï¼‰ã€‚")

            # 2) åµæ¸¬ comm_size <= 0
            if detailed_info['comm_sizes'] and any(sz <= 0 for sz in detailed_info['comm_sizes']):
                print("      âŒ åµæ¸¬åˆ° comm_size <= 0ï¼›è«‹åœ¨è½‰æª”å¾Œè£œå€¼ï¼ˆgroup_id / comm_sizeï¼‰ã€‚")
            elif not detailed_info['comm_sizes']:
                print("      âš ï¸  æœ¬æª”æ¡ˆç¼ºå°‘ COMM å°ºå¯¸çµ±è¨ˆï¼Œå¯èƒ½ä»£è¡¨ COMM å±¬æ€§æœªæ­£ç¢ºå¯«å…¥ã€‚")

        if args.validate:
            all_detailed_info.append((rank, detailed_info))

    # æ·±åº¦é©—è­‰
    if args.validate and len(all_detailed_info) > 1:
        print(f"\nğŸ” æ·±åº¦é©—è­‰åˆ†æ:")

        # æª¢æŸ¥ ranks é–“çš„ä¸€è‡´æ€§
        ranks = [info[0] for info in all_detailed_info]
        node_counts = [info[1]['compute_nodes'] + info[1]['comm_nodes'] for info in all_detailed_info]
        comm_counts = [info[1]['comm_nodes'] for info in all_detailed_info]

        # æ™ºèƒ½åˆ†æç¯€é»æ•¸å·®ç•°
        print(f"   ç¯€é»æ•¸åˆ†ä½ˆ: {node_counts}")
        if len(set(node_counts)) == 1:
            print("   âœ… æ‰€æœ‰ ranks çš„ç¯€é»æ•¸ç›¸åŒ")
        else:
            max_nodes = max(node_counts)
            min_nodes = min(node_counts)
            diff = max_nodes - min_nodes
            diff_pct = (diff / max_nodes) * 100

            print(f"   ğŸ“Š ç¯€é»æ•¸å·®ç•°åˆ†æ:")
            print(f"      æœ€å¤§: {max_nodes}, æœ€å°: {min_nodes}, å·®ç•°: {diff} ({diff_pct:.1f}%)")

            # åˆ¤æ–·å·®ç•°æ˜¯å¦åˆç†
            if diff <= max_nodes * 0.05:  # å°æ–¼5%çš„å·®ç•°
                print("   âœ… ç¯€é»æ•¸å·®ç•°åœ¨åˆç†ç¯„åœå…§ (â‰¤5%)")
                print("      å¯èƒ½åŸå› : æ•¸æ“šåˆ†å‰²ä¸å‡ã€å‹•æ…‹åœæ­¢ã€ç¶²è·¯åŒæ­¥å»¶é²")
            elif diff <= max_nodes * 0.15:  # å°æ–¼15%çš„å·®ç•°
                print("   âš ï¸  ç¯€é»æ•¸å·®ç•°è¼ƒå¤§ä½†å¯æ¥å— (5-15%)")
                print("      å»ºè­°æª¢æŸ¥: æ•¸æ“šåŠ è¼‰å™¨é…ç½®ã€sampler è¨­å®š")
            else:
                print("   âŒ ç¯€é»æ•¸å·®ç•°éå¤§ (>15%)")
                print("      å¯èƒ½å•é¡Œ: è¨“ç·´æå‰çµ‚æ­¢ã€åš´é‡çš„åŒæ­¥å•é¡Œ")

        # é€šè¨Šç¯€é»åˆ†æ
        print(f"   é€šè¨Šç¯€é»æ•¸: {comm_counts}")
        if len(set(comm_counts)) == 1:
            print("   âœ… æ‰€æœ‰ ranks çš„é€šè¨Šç¯€é»æ•¸ç›¸åŒ")
        else:
            comm_diff = max(comm_counts) - min(comm_counts)
            print(f"   ğŸ“Š é€šè¨Šç¯€é»å·®ç•°: {comm_diff}")
            if comm_diff == diff // 2:  # æ¯æ­¥åŒ…å«1å€‹è¨ˆç®—+1å€‹é€šè¨Šç¯€é»
                print("   âœ… é€šè¨Šç¯€é»å·®ç•°èˆ‡ç¸½ç¯€é»å·®ç•°ä¸€è‡´ (æ­£å¸¸)")
            else:
                print("   âš ï¸  é€šè¨Šç¯€é»å·®ç•°èˆ‡é æœŸä¸ç¬¦ï¼Œå¯èƒ½æœ‰ç•°å¸¸")

        # æ­¥æ•¸æ¨ç®—
        steps = [count // 2 for count in node_counts]  # å‡è¨­æ¯æ­¥2å€‹ç¯€é»
        print(f"   æ¨ä¼°æ­¥æ•¸: {steps}")
        if len(set(steps)) > 1:
            step_diff = max(steps) - min(steps)
            print(f"   ğŸ“Š æ­¥æ•¸å·®ç•°: {step_diff} æ­¥")
            print(f"      é€™åœ¨åˆ†æ•£å¼è¨“ç·´ä¸­æ˜¯æ­£å¸¸ç¾è±¡")

        # æª¢æŸ¥é€šè¨Šå¤§å°ä¸€è‡´æ€§
        all_comm_sizes = [set(info[1]['comm_sizes']) for info in all_detailed_info]
        if len(all_comm_sizes) > 1 and all(sizes == all_comm_sizes[0] for sizes in all_comm_sizes):
            print("   âœ… æ‰€æœ‰ ranks çš„é€šè¨Šå¤§å°ä¸€è‡´")
        else:
            print("   âš ï¸  ä¸åŒ ranks çš„é€šè¨Šå¤§å°å¯èƒ½ä¸ä¸€è‡´")

    # é€£çºŒå‘½åæª¢æŸ¥ï¼šrank æ‡‰ç‚ºé€£çºŒï¼ˆ0..N-1ï¼‰
    idxs = [int(PATTERN.match(p.name).group(2)) for p in files]  # group(2) æ˜¯æ•¸å­—éƒ¨åˆ†
    expect = list(range(min(idxs), min(idxs) + len(idxs)))
    if sorted(idxs) != expect:
        print(f"[WARN] æª”å index ä¸æ˜¯é€£çºŒçš„ï¼š{sorted(idxs)}")
    else:
        print(f"âœ… æª”å index é€£çºŒæ€§æ­£ç¢º: {sorted(idxs)}")


if __name__ == "__main__":
    main()
